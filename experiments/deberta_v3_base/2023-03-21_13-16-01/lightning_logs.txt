GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                  | Type                 | Params
---------------------------------------------------------------
0 | bert_model            | DeBERTEncoder        | 183 M 
1 | classifier            | TokenClassifier      | 594 K 
2 | loss                  | CrossEntropyLoss     | 0     
3 | classification_report | ClassificationReport | 0     
---------------------------------------------------------------
184 M     Trainable params
0         Non-trainable params
184 M     Total params
737.704   Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=5` reached.
Restoring states from the checkpoint path at experiments/deberta_v3_base/2023-03-21_13-16-01/checkpoints/deberta_v3_base--f1=97.5666-epoch=4.ckpt
Restored all states from the checkpoint file at experiments/deberta_v3_base/2023-03-21_13-16-01/checkpoints/deberta_v3_base--f1=97.5666-epoch=4.ckpt
